---
layout: home
# Add any other page-specific variables here, like a title
title:
---

My recent projects deconstruct LLM components by implementing them myself and explaining the underlying theory:

[**[Transformer LM]**](https://github.com/xycoord/Language-Modelling/) | From-scratch implementation featuring RoPE and KV cache, emphasis on code quality and clarity
[**[RL Course]**](https://github.com/xycoord/deep-rl-course) | Teaching RL through rigorous mathematical derivations and implementations from first principles
[**[BPE Tokeniser]**](./Optimising-BPE) | Optimised training implementation (hours → 13s) with detailed technical writeup

I'm also exploring [**[Mechanistic Interpretability]**](https://github.com/xycoord/Language-Modelling/tree/main/src/mech_interp) through toy model experiments: reproducing superposition research and training SAEs to extract learned features.

*Oxford MCompPhil • Computer Science + Philosophy of Mind & Ethics*  
*First Class (2024)*
